---
title: "Conducting EDA on Corruption Data"
author: "Andre Fernandes, Keenan Szulik, and Erik Hou"
date: "`r format(Sys.time(),'%m/%d/%Y')`"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<!-- ################################### -->
<!-- # Erik's section Begins -->
<!-- ################################### -->

## Introduction

This analysis is motivated by the following research question:

\begin{center}
"If there is any relationship between corruption and parking violations both pre and post 2002 and if there are any other relevant explanatory variables."
\end{center}

The question will be addressed by exploratory data analysis techniques. We are asked to imagine that we have been hired by the World Bank to explore the influence cultural norms and legal enforcement have on controlling corruption. To operationalize the analysis, the assignment looks at the parking behavior of United Nations officials in Manhattan.

Until 2002, UN diplomats are protected by diplomatic immunity; therefore, they were not subject to parking enforcement actions and their actions were solely constrained by cultural norms. In 2002, the parking authority acquired the right to confiscate diplomatic license plates of the violateors. As a result, their parking behavior was constrained by both cultural norms and the legal penalties.

We are given a dataset of a selection of UN diplomatic missions, which includes target variable, *violations*, some essential variables for the analysis, such as country codes, label for pre and pos the parking enforcement change, the corruption index, and other variables.

<!-- ################################### -->
<!-- # Maybe include some methodology and caveats here based on how we decided to approach the analysis -->
<!-- ################################### -->

### Setup

First, we load some of the packages we will need for the analysis and the data into R.

Load the data:
```{r, echo=TRUE}
source("utils/functions.R")
df <- load_rda('data/Corrupt.Rdata')
```

### Overview of the data structure

We have `r nrow(df)` observations.
```{r}
nrow(df)
```

We have `r length(colnames(df))` variables in the dataset:
```{r}
str(df)
```

Look at the summary of the dataset:
```{r}
summary(df)
```

Examine the first ten rows of the dataset:
```{r}
head(df, 10)
```

### Data Selection and Cleaning

From examining the data summary and the first ten rows, we see  many NA values in the key variable fields, such as violations and corruption. Also notice that, in the field prepost, we have blanks. 

It is necessary to clean the data by taking out the records with the essential fields being blank or NA before starting analysis on the variables:

```{r}
df[df=="" | df=="NA"] = NA  #set all the blanks and "NA" to NA

#exlcude the records having NAs in at least one of the essential fields
df_clean = subset(df, !is.na(wbcode) & !is.na(prepost) & !is.na(violations) & !is.na(corruption))

```

One last step before starting univariate analysis of key variables is to make sure in our cleaned dataset, we have exactly two records per country, one pre and one post 2002. Because,
1. With a missing pre or pos record, it would be difficult to make comparisons of countries' behavior pre and post the policy change. 
2. Also, if we had some countries having more than one pre and/or one post record, further cleaning or manipulation would have to take place to make sure we appropriately weigh different observations.


```{r}
nrow(df_clean)  #the total number observations in the data set
length(unique(df_clean$wbcode))  #the total number of unique countries
length(unique(df_clean[df_clean$prepost == "pre",]$wbcode))  #the total number of distinct countries in the data set with prepost == "pre"
length(unique(df_clean[df_clean$prepost == "pos",]$wbcode))  #the total number of distinct countries in the data set with prepost == "pos"

```
From the above numbers, we know that we have a total of `r nrow(df_clean)` observations left in our dataset after cleaning with `r length(unique(df_clean$wbcode))` different counties. Further checking for integrity, we note that, in the dataset, `r length(unique(df_clean[df_clean$prepost == "pre",]$wbcode)) ` unique coutries having prepost field being "pre" and `r length(unique(df_clean[df_clean$prepost == "pos",]$wbcode)) ` unique coutries having prepost field being "pos". Finally, we are sure now that we have a clean enough dataset to start subsequent analysis  


## Univariate Analysis of Key Variables

Now We start the univariate analysis.

### Variable, Violations

First is to look at the variable, violations:

```{r, echo=TRUE}
summary(df_clean$violations)
sd(df_clean$violations)
hist(df_clean$violations,20,xlab = "Violations", main = "Histogram of Violations")
```

There are several features of the variables worth highlighting:

1. All the values are non-negative.
2. From both the histogram and the numeric summary, we can see that the values are very clustered to the lower end where more than 50% of the values are less than 6.
3. The distribution is right-skewed with some really big outliers which causes the mean to be greater than the median and relatively high standard deviation, about 302.

Since there are `r nrow(df_clean[df_clean$violations == 0,])` zeros with many datapoints clustering close to zero with only a few outliers taking much greater values. Try to draw a histogram of $log(volations + 1)$ to help us better visualize the distribution. While drawing the histogram, we adjust the position of the bins so the first bar is centered around zero. 
```{r}
nrow(df_clean[df_clean$violations == 0,])
```
```{r}
hist(log(df_clean$violations+1),breaks = seq(-0.75,10,0.5)+0.5, ylim = c(0,50), xlab = "log(Violations+1)", main = "Histogram of Violations")
```

Notice that the frequency distribution has two modes one in (-0.25,0.25) and one in (4.25,4.75). This probably is caused by the change of policy where there were more violations before 2002 and less violation after 2002.

Two histograms of violations before and after the change of policy prove the assumption.
```{r}
hist(log(df_clean[df_clean$prepost == "pre",]$violations+1),breaks = seq(-0.75,10,0.5)+0.5, ylim = c(0,50), xlab = "log(Violations+1)", main = "Histogram of Violations until 2002")
hist(log(df_clean[df_clean$prepost == "pos",]$violations+1),breaks = seq(-0.75,10,0.5)+0.5, ylim = c(0,50), xlab = "log(Violations+1)", main = "Histogram of Violations starting 2002")
```

From this above drawing, we demonstrated the shift in the distribution of violation before and after the policy change.
**!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!do we want to use boxplot??!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!**


### Variable, Corruption

Next, we move on to the other key variable, corruption.

First step is to look at the numeric summary of the variable and its histogram:

```{r}
summary(df_clean$corruption)
sd(df_clean$corruption)
hist(df_clean$corruption, breaks = 20, xlab = "Corruption", main = "Histogram of Corruption")
axis(1, at = seq(-3,2,by=0.5), labels = seq(-3,2,by=0.5))
```

We draw the histograms of corruption of pre and post the policy change separately to compare:


```{r}
hist(df_clean[df_clean$prepost == "pre",]$corruption, breaks = 20, xlab = "Corruption", main = "Histogram of Corruption until 2002")
axis(1, at = seq(-3,2,by=0.5), labels = seq(-3,2,by=0.5))
hist(df_clean[df_clean$prepost == "pos",]$corruption, breaks = 20, xlab = "Corruption", main = "Histogram of Corruption starting 2002")
axis(1, at = seq(-3,2,by=0.5), labels = seq(-3,2,by=0.5))

```

We notice that the two histograms seem identical, which means most likely, the dataset have corruption as a constant over time. Just to double-check, we test if for each country the corruption is the same pre and post 2002.

```{r}
nrow(unique(df_clean[,c("wbcode", "corruption")]))
```

Draw a boxplot of corruption:

```{r}
boxplot(unique(df_clean[,c("wbcode", "corruption")])$corruption, ylab = "corruption")
```


Several key feature of the variable, corruption:

1. Through making sure that the number of unique combinations of wbcode and corruption is the same as the number of unique wbcodes. We are sure that the dataset has corruption as a constant for each country.
2. We can see that the histogram has two modes one around 0.75 and one around -2.5.
3. The distribution is left-skewed with most countries having the values between 0 and 1 and not too few outliers cluster close to the second mode.
4. For corruption, it might be interesting to see if the outliers themselves share some characteristics in common compared to other countries in the dataset.


<!-- ################################### -->
<!-- # Andre's section Begins -->
<!-- ################################### -->

## Analysis of Key Relationships

In this section, we will be conducting multivariate analysis on our corruption data. This section will be divided into two segments comprised of __correlations for numerical variables__ and a deeper dive into variable relationships while observing __prepost__ variable.

### Analyzing correlations among continuous variables

#### All line items

Here, we do a quick look at correlation of the numerical variables. It appears that most data is static between the two snapshot dates (as mentioned above).

1. Violations and fines are perfectly correlated, so they tell us the same information about the data. We will check to see if they are identical later in this analysis.
2. Since we know that the prepost variable captures a time element, we will be viewing the correlations in each of the _pre_ and _pos_ subset groups
3. Staff, Spouse, and Car Total have a high positive correlation, which makes sense when we think of the semantic meaning of these variables.
4. GDP and Corruption have a very high negative correlation. This is not a surprising relationship since it's a common hypothesis in the field.
5. There are many other relationships to look at, but for the sake of brevity, we will end the analysis into the combined data correlations here.


_The plot_correlation function was moved to the functions.R file_

```{r, echo=TRUE, fig.width=13, fig.height=7}

# all lines
plot_correlation(df_clean, 10)

```

#### Pre and Post 2002

1. The interesting observation from comparing __pre__ and __post__ 2002 correlations is that __milaid__ and __totaid__ show relatively high positive correlations with __violations__ and __fines__. During the __post 2002__ period, that correlation is very weak.
2. There's a shortage of information about the data, so we will not be able to explain why the correlations are so different between the beforementioned variables during the two snapshots of time

Here, we will subset out data to _prepost == 'pre'_.

```{r, echo=TRUE, fig.width=13, fig.height=7}

# pre lines
plot_correlation(df_clean %>% filter(prepost=='pre'), 10)

```

Here, we will subset out data to _prepost == 'pos'_.

```{r, echo=TRUE, fig.width=13, fig.height=7}

# post lines
plot_correlation(df_clean %>% filter(prepost=='pos'), 10)

```

#### Looking further into the relationship between violations and fines

We observed a very strong correlation between __violations__ and __fines__, so now let's plot a scatterplot with these two variables and add the bet fit line. Please note that we saw the correlation metrics prior to even adding a log transformation, so we will keep the variables as is for this graph.

The variables indeed appear to be perfectly correlated. This makes sense since fines are likely violations multiplied by a scalar.
```{r, echo=TRUE, fig.width=13, fig.height=7}

# regular
plot_scatter <- ggplot(df_clean, aes(x=fines, y=violations)) + 
                    geom_point()+ geom_smooth(method=lm,  linetype="dashed", color="darkred", fill="blue")+ 
                    geom_text(x = 40000, y = 2000, label = lm_eqn(df_clean, 'fines', 'violations'), parse = TRUE)+
                    labs(title = "Scatter Plot for Violations and Fines with Linear Model")
plot(plot_scatter)

```

### Deep dive into variable relationships with violations while considering pre-post 2002 timestamp

In this section, we will analyze variables that we believe are important. We will not cover all variable for brevity.

#### Region

One of the key categorical variables to observe is __region__. Although the provided data set only provides an integer factor for the regions, we were able to use the regional indicators to map the integer factors to actual region names. Also, please note that the African country of __Zaire__ had a missing value for __region__. Sice we were able to do a quick internet search and discovered that __Zaire__ is in __Africa__, we fixed the data when creating the __region_clean__ variable.

What we were able to observe:

1. North America region has the lowest mean __log(violations+1)__ before and after 2002.
2. Middle East region has the largest interquartile range before and after 2002.
3. The target variable is much lower after 2002 (as we would expect since countries are now paying their fines, which lowers the total upaid level)

```{r, echo=TRUE, fig.width=13, fig.height=7}

df_clean$region_clean <- ifelse(is.na(df_clean$region),6,df_clean$region)
df_clean$region_clean <- as.factor(df_clean$region_clean)
levels(df_clean$region_clean) <- c('n_america', 's_america', 'europe', 'asia', 'oceania', 'africa', 'mid_east')

plot_vars(df_clean, 'violations', 'region_clean', 'cat', 'prepost')

```

#### Corruption

One of the key numerical variables to observe is __corruption__. 

What we were able to observe:

1. Corruption has a more positive relationship with the target variable before 2002 than after.
2. The value for the corruption variable is static, so they are not different between the two snapshots in time. It is strange to want this variable to be static since we would expect that the corruption index changes over time.

```{r, echo=TRUE, fig.width=13, fig.height=7}

plot_vars(df_clean, 'violations', 'corruption', 'numeric', 'prepost')

```

#### Fines

Another key numerical variables to observe is __fines__. We already plotted the relationship between violations and fines earlier, so here we take a look at the relationship between the log transformations of the two variables.

What we were able to observe:

1. There still remains a clear relationsip between the log transformations of the two variables.
2. The relationships appear to be less linear than before, but the linear estimator is still okay in this case.
3. If a predictive model were built to predict violations, this is a classic example of data leakage. If you attempt to use __fines__ as a predictor for __violations__, you would not have __fine__ information at the time of prediction. This means that this variable is only available in our data because we are looking at historical data. 


```{r, echo=TRUE, fig.width=13, fig.height=7}
df_clean$log_fines <- log(df_clean$fines +1)
plot_vars(df_clean, 'violations', 'log_fines', 'numeric', 'prepost')

```

#### Majority Muslim coutry indicator

We are provided in the data set variables for __pctmuslim__ and __majoritymuslim__, which are respectively percent of population that is muslim and a flag that captures whether or not a country is majority muslim. Unfortunately, the levels for the __majoritymuslim__ are not intuitive from the information we were provided, so we created our own flag using the __pctmuslim__ variable.

What we were able to observe:

1. There are a lot more countries in these UN events that were not majority muslim than yes.
2. We don't learn much information about this variable. Majority muslim appear to have higher log violations, but that different is so small that it is likely due to noise. Majority muslim countries also represent less datapoints in our data, which makes any aggregate information even more succeptible to noise.

```{r, echo=TRUE, fig.width=13, fig.height=7}
df_clean$maj_musl_ind <- ifelse(is.na(df_clean$pctmuslim), 'missing', ifelse(df_clean$pctmuslim>.5,'yes','no'))
plot_vars(df_clean, 'violations', 'maj_musl_ind', 'cat', 'prepost')


```

#### Trade

What we were able to observe:

1. The relationship between trade and the log transformation of violation is all over the place. We would not trust the linear models in this case.

```{r, echo=TRUE, fig.width=13, fig.height=7}

df_clean$log_trade <- log(df_clean$trade+1)
plot_vars(df_clean, 'violations', 'log_trade', 'numeric', 'prepost')


```

#### Total Cars

What we were able to observe:

1. There a positive relationship between number of total cars and the log transformation of violations. This makes intuitive sense.

```{r, echo=TRUE, fig.width=13, fig.height=7}

plot_vars(df_clean, 'violations', 'cars_total', 'numeric', 'prepost')


```


<!-- ################################### -->
<!-- # Keenan's section Begins -->
<!-- ################################### -->

### Analysis of Secondary Effects

Fill in with information.

```{r, echo=TRUE}

```

### Conclusion

You can also embed plots, for example:

```{r, echo=TRUE}

```

<style>
    body .main-container {
        max-width: 1600px;
    }
</style>